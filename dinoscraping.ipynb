{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec259c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of links to process: 3\n",
      "Processing link 1 of 3...\n",
      "Found link: https://www.greatschools.org/indiana/crown-point/301-Lake-Street-Elementary-School/ (Time taken: 4.095099210739136 seconds)\n",
      "Found contact info: ([], ['(219) 663-5683', '(219) 662-4329'], 'https://www.greatschools.org/indiana/crown-point/301-Lake-Street-Elementary-School/#Neighborhood') (Time taken: 5.782843112945557 seconds)\n",
      "Processing link 2 of 3...\n",
      "Found link: https://www.vonsteuben.org/ (Time taken: 4.214574098587036 seconds)\n",
      "Found contact info: (['khallberg@cps.edu'], ['(773) 534-5100', '(773) 534-5210'], 'https://www.vonsteuben.org/apps/contact/') (Time taken: 3.544929027557373 seconds)\n",
      "Processing link 3 of 3...\n",
      "Found link: https://www.hancockhs.org/ (Time taken: 2.9348411560058594 seconds)\n",
      "Found contact info: ([], ['(773) 535-2410'], 'javascript:;') (Time taken: 4.002914905548096 seconds)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import threading\n",
    "import urllib.parse\n",
    "import logging\n",
    "\n",
    "# Proxy settings\n",
    "username = 'user-splx70gleh-country-us'\n",
    "password = 'h4ui90lsvyZOS2chAr'\n",
    "proxy = f\"socks5h://{username}:{password}@gate.smartproxy.com:7000\"\n",
    "proxies = {\n",
    "    'http': proxy,\n",
    "    'https': proxy\n",
    "}\n",
    "\n",
    "# Load the xls file\n",
    "df = pd.read_excel('list_data.xlsx')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraping_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Define a function to generate email guesses\n",
    "def guess_emails(business_name):\n",
    "    # Remove non-alphanumeric characters\n",
    "    clean_name = re.sub(r'\\W+', '', business_name)\n",
    "    return f'{clean_name}@gmail.com, info@{clean_name}.com'\n",
    "\n",
    "# Define a function to search Google\n",
    "def google_search(company_name):\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    url = 'https://www.google.com/search'\n",
    "    try:\n",
    "        response = requests.get(url, params={'q': company_name}, proxies=proxies)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Problem with request: {str(e)}\")\n",
    "        return None\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('div', class_='kCrYT')\n",
    "        first_link = None\n",
    "        for link in links:\n",
    "            a_tag = link.find('a')\n",
    "            if a_tag is None:  \n",
    "                continue\n",
    "            url = a_tag.get('href')  \n",
    "            url = re.sub(r\"/url\\?q=([^&]*)&sa=.*\", r\"\\1\", url)\n",
    "            if any(substring in url.lower() for substring in [\"orginization\", \"company\",\"thebluebook\",\"profile\",\"directory\",\"directories\",\"zoom\",\"info\",\"indeed\",\"vendor\",\"companies\",\"yelp\",\"youtube\", \"google\", \"wikipedia\", \"facebook\", \"instagram\", \"linkedin\", \"twitter\"]):\n",
    "                continue\n",
    "\n",
    "            # Split the company name into words of length greater than 5\n",
    "            words = [word for word in company_name.split() if len(word) > 5]\n",
    "\n",
    "            # Get the URL's domain\n",
    "            domain = urllib.parse.urlparse(url).netloc\n",
    "\n",
    "            # If any word from the company name appears in the domain, choose this URL\n",
    "            if any(word.lower() in domain.lower() for word in words):\n",
    "                return url\n",
    "            if first_link is None:\n",
    "                first_link = url\n",
    "        return first_link if first_link is not None else \"No relevant link found\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Problem parsing response: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define a function to scrape a website for contact information\n",
    "def scrape_website(url):\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    try:\n",
    "        response = requests.get(url, proxies=proxies)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Problem with request: {str(e)}\")\n",
    "        return None\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "        email_matches = re.findall(email_pattern, text)\n",
    "        phone_matches = re.findall(phone_pattern, text)\n",
    "        contact_link = None\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            if \"contact\" in a_tag.text.lower():\n",
    "                contact_link = urllib.parse.urljoin(url, a_tag['href'])\n",
    "                break\n",
    "        return email_matches, phone_matches, contact_link\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Problem parsing response: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Define a wrapper function to call google_search with a timeout\n",
    "def google_search_with_timeout(row, timeout=60):\n",
    "    result = [None]\n",
    "    def job():\n",
    "        result[0] = google_search(row['Legal Business Name'])\n",
    "    thread = threading.Thread(target=job)\n",
    "    thread.start()\n",
    "    thread.join(timeout=timeout)\n",
    "    if thread.is_alive():\n",
    "        logging.error(f\"Timeout: google_search took longer than {timeout} seconds.\")\n",
    "        return None\n",
    "    else:\n",
    "        return result[0]\n",
    "\n",
    "# Similar wrapper for scrape_website\n",
    "def scrape_website_with_timeout(url, timeout=60):\n",
    "    result = [None]\n",
    "    def job():\n",
    "        result[0] = scrape_website(url)\n",
    "    thread = threading.Thread(target=job)\n",
    "    thread.start()\n",
    "    thread.join(timeout=timeout)\n",
    "    if thread.is_alive():\n",
    "        logging.error(f\"Timeout: scrape_website took longer than {timeout} seconds.\")\n",
    "        return None\n",
    "    else:\n",
    "        return result[0]\n",
    "\n",
    "# Prompt the user for the number of links to process\n",
    "num_links = int(input(\"Enter the number of links to process: \"))\n",
    "\n",
    "# Iterate over the DataFrame\n",
    "for index, row in df.head(num_links).iterrows():\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing link {index+1} of {num_links}...\")\n",
    "    link = google_search_with_timeout(row)\n",
    "    if link is None:  # Skip this row if google_search took too long\n",
    "        continue\n",
    "    print(f\"Found link: {link} (Time taken: {time.time() - start_time} seconds)\")\n",
    "    df.loc[index, 'Link'] = link\n",
    "    start_time = time.time()\n",
    "    contact_info = scrape_website_with_timeout(link)\n",
    "    if contact_info is None:  # Skip this row if scrape_website took too long\n",
    "        continue\n",
    "    print(f\"Found contact info: {contact_info} (Time taken: {time.time() - start_time} seconds)\")\n",
    "    email_matches = ', '.join(contact_info[0]) if contact_info[0] else guess_emails(row['Legal Business Name'])\n",
    "    df.loc[index, 'Emails'] = email_matches\n",
    "    df.loc[index, 'Phone Numbers'] = ', '.join(contact_info[1]) if contact_info[1] else ''\n",
    "    df.loc[index, 'Contact Form Link'] = contact_info[2] if contact_info[2] else ''\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# Save the DataFrame back to an xls file\n",
    "df.to_excel('schoolInfo.xlsx', index=False)\n",
    "print(\"Done!\")\n",
    "#run on 450 links to be safe theres only 467 or something in the original business names excel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
